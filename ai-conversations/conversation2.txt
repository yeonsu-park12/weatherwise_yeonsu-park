ğŸ“‚Conversation2. Improving LLM Utility Function Robustness and Defining Modularity

----------------------------------
Tool: ChatGPT, Google Gemini
Date: 2025-10-14
----------------------------------

Me: Before we dive into coding, I have a few questions. First, what is the Ollama application, and why would I need to use it? Do you recommend it?

AI: Ollama is essentially a local or cloud-based LLM (Large Language Model) server that allows your applications to communicate with AI models programmatically through an API. Using it in WeatherWise lets you query an AI model for text generation, weather explanations, or any automated reasoning you want. Iâ€™d recommend it if you need a controllable, programmable LLM environment, especially for prototyping AI features without depending on external APIs like OpenAI or HuggingFace.

Me: Got it. That makes sense. Second question: how can I better understand API functions and the underlying code? Sometimes I see a function and I donâ€™t know exactly whatâ€™s happening.

AI: A good approach is to break it down step by step:

Look at the function signature â€“ it tells you the inputs and outputs.

Identify the external calls (like requests.post) and understand what each parameter does.

Check return values and how exceptions are handled.

Use small test scripts or print statements to see intermediate outputs. Basically, treat it like a â€œblack boxâ€ you can probe gradually until itâ€™s clear.

Me: Thatâ€™s helpful. Lastly, how can I adjust coding well, and how can I work with that effectively?

AI: Iterative development is key. Start with a minimal working version. Then:

Refactor incrementally (modularize functions, handle edge cases).

Use comments and pseudocode to plan logic before coding.

Test frequently with both normal and edge case inputs.

Donâ€™t hesitate to ask â€œwhyâ€ each line exists â€“ understanding intent is more important than memorizing syntax.

Me: Perfect. Now that I understand why weâ€™re using Ollama, how to read API functions, and how to adjust code properly, letâ€™s start improving the ask_ollama function step by step.

AI: Excellent. Letâ€™s begin by analyzing the current limitations of your basic ask_ollama_basic function. First, it does not handle streaming responses, which means the user sees the entire response only after the model finishes generating it. This can create a noticeable delay for longer outputs. Second, the error handling is minimal â€” it simply catches a RequestException without differentiating between network failures, HTTP errors, or other unexpected exceptions. Both issues reduce robustness and usability.

Me: I see. So we need a streaming solution to improve perceived responsiveness, and a more granular error handling system. Could you explain how we would implement streaming with requests.post?

AI: Sure. The idea is to set stream=True in the POST request. Then you can iterate over response.iter_lines(), which gives you each line of output as it arrives. Each line usually contains a JSON chunk representing partial LLM output. We parse it using json.loads, extract chunk["message"]["content"], and append it to a cumulative output string. The loop stops when the chunk contains "done": True.

Me: What happens if one of the lines is not valid JSON?

AI: Thatâ€™s an edge case. In a robust implementation, we catch json.JSONDecodeError and simply skip the malformed line. This ensures the stream continues without crashing.

Me: Great. Now, regarding error handling, can we separate network failures from HTTP status code errors more explicitly?

AI: Absolutely. If response.raise_for_status() fails, it raises an HTTPError, which is a subclass of RequestException. If the HTTPError object has a response attribute, we can access response.status_code and response.reason to return a detailed error message. Pure network failures, like DNS resolution or connection timeout, wonâ€™t have a response object, so we can return a different message indicating a network issue.

Me: So, in practice, weâ€™d have three categories: successful streaming output, HTTP errors with status codes, and network-level exceptions.

AI: Correct. And for completeness, we should also include a generic except Exception block to catch any unexpected errors, like JSON parsing failures in the final aggregation step. This ensures your application never crashes due to an unhandled edge case.

Me: Understood. Once streaming and error handling are integrated, how should we structure the application to maintain modularity?

AI: We should follow the Single Responsibility Principle. Your WeatherWise app can be separated into three layers:

Data Acquisition Layer: get_weather_data â€“ responsible only for fetching raw weather JSON and handling network/API errors.

Data Processing Layer: parse_weather_data â€“ takes raw JSON, extracts relevant information for visualization or AI input.

Application/UI Layer: run_app â€“ coordinates the workflow, calls the other two layers, and interacts with ask_ollama for AI responses.

This separation ensures that ask_ollama focuses purely on LLM communication and streaming, while get_weather_data handles network robustness.

Me: Thatâ€™s very clear. For streaming, do we want to provide partial outputs to the UI as each chunk arrives, or just return the full string at the end?

AI: For a console prototype, accumulating the output and returning it at the end is sufficient. But for a GUI or web interface, youâ€™d want to yield each chunk or call a callback function to display content in real-time. This improves user experience, especially for long LLM responses.

Me: Makes sense. Now, about testing â€” how can I make sure ask_ollama behaves correctly under all these edge cases?

AI: You can simulate different scenarios:

Normal streaming: Mock a server response with multiple JSON chunks and "done": True.

Malformed JSON chunks: Inject a line that is not valid JSON to see if your function skips it gracefully.

HTTP errors: Use a test server that returns 404 or 500 status codes.

Network errors: Temporarily disconnect network or mock requests.post to raise ConnectionError or Timeout.

Each test should assert that the function either returns the full accumulated content or an appropriate error message dictionary/string.

Me: Excellent. So, in summary, we will:

Refactor ask_ollama_basic to ask_ollama with streaming support.

Implement granular error handling for network, HTTP, and unexpected errors.

Keep the function modular, focusing only on LLM communication.

Use SRP to structure the whole app into three layers.

Test all edge cases rigorously.

AI: Exactly. Following this approach, your WeatherWise app will have a robust, modular, and maintainable LLM utility function that works reliably in production and development environments.

Me: Perfect. I feel confident now to implement ask_ollama fully with streaming and robust error handling.

AI: Great. Once implemented, you can integrate it into run_app, and it will seamlessly provide AI-powered weather insights while remaining maintainable and testable. 

Me: I want to improve the LLM utility function ask_ollama for WeatherWise. The initial version works for basic requests but it doesn't handle streaming responses and network errors well. Can we go step by step to make it more robust and modular?

AI: Absolutely. Letâ€™s start by analyzing the current limitations. Can you share your existing function?

Me: Sure, hereâ€™s the basic version:

def ask_ollama_basic(messages: list) -> str:
    url = f"{OLLAMA_SERVER}/api/chat"
    headers = {"Content-Type": "application/json"}
    data = {"model": MODEL_NAME, "messages": messages, "stream": False}
    
    try:
        response = requests.post(url, headers=headers, json=data, timeout=45)
        response.raise_for_status()
        content = response.json()['message']['content']
        return content.strip()
    except RequestException as e:
        return f"âŒ Basic Error during LLM call: {type(e).__name__}"


AI: I see. The main issues here are:

It assumes stream=False only. No support for streaming incremental responses.

Error handling is very general; it wonâ€™t give detailed HTTP status codes or separate network errors from other exceptions.

Me: Exactly. First, I want to tackle streaming responses. I want each chunk to be appended to the output and stop when the server marks the stream as done. How would you approach that?

AI: We can use response.iter_lines() to read the streamed content line by line. Each line can be parsed as JSON. If it contains chunk['message']['content'], append it to the output. Stop when chunk.get("done", False) is True. Also, any malformed JSON lines should be skipped to avoid crashing.

Me: That makes sense. Can you show a refactored function skeleton for this?

AI: Sure. Hereâ€™s a structured approach:

def ask_ollama(messages: list) -> str:
    url = f"{OLLAMA_SERVER}/api/chat"
    headers = {"Content-Type": "application/json"}
    data = {"model": MODEL_NAME, "messages": messages}

    try:
        response = requests.post(url, headers=headers, json=data, stream=True, timeout=45)
        response.raise_for_status()
        
        output = ""
        for line in response.iter_lines():
            if line:
                try:
                    chunk = json.loads(line.decode("utf-8"))
                    if "message" in chunk and "content" in chunk["message"]:
                        output += chunk["message"]["content"]
                    if chunk.get("done", False):
                        break
                except json.JSONDecodeError:
                    continue
        return output.strip()
    except requests.exceptions.RequestException as e:
        return f"âŒ Network or HTTP Error during LLM call: {type(e).__name__}"
    except Exception as e:
        return f"âŒ Unexpected LLM Error: {type(e).__name__}"


Me: Thatâ€™s great for streaming. But I want to also improve the error handling. For example, if response.raise_for_status() triggers an HTTP error, I want to return the status_code and reason. How can we integrate that cleanly?

AI: You can check if e has a response attribute. If yes, extract e.response.status_code and e.response.reason. For network issues like timeout or DNS failure, you return a different error message. Hereâ€™s an improved version:

def ask_ollama(messages: list) -> str:
    if not OLLAMA_SERVER or not MODEL_NAME:
        return "âŒ Error: AI server or model is not configured."

    url = f"{OLLAMA_SERVER}/api/chat"
    headers = {"Content-Type": "application/json"}
    data = {"model": MODEL_NAME, "messages": messages}

    try:
        response = requests.post(url, headers=headers, json=data, stream=True, timeout=30)
        response.raise_for_status()

        output = ""
        for line in response.iter_lines():
            if line:
                try:
                    chunk = json.loads(line.decode("utf-8"))
                    if "message" in chunk and "content" in chunk["message"]:
                        output += chunk["message"]["content"]
                    if chunk.get("done", False):
                        break
                except json.JSONDecodeError:
                    continue

        return output.strip()

    except requests.exceptions.RequestException as e:
        if hasattr(e, 'response') and e.response is not None:
            return f"âŒ Error: {e.response.status_code} {e.response.reason} for url: {url}"
        return f"âŒ Network/Connection Failure: {type(e).__name__}"
    except Exception as e:
        return f"âŒ Unexpected error in ask_ollama: {type(e).__name__}"


Me: Excellent. This separates HTTP errors from network failures clearly. Now, thinking about modularity, I want the application to have clear separation of concerns. What would a good architecture look like for run_app, get_weather_data, and this LLM utility?

AI: Following the Single Responsibility Principle (SRP), the app can be divided into three layers:

Data Acquisition Layer: get_weather_data handles all external API calls and network errors.

Data Processing Layer: parse_weather_data (to be implemented) takes raw JSON and extracts key metrics for visualization or AI consumption.

Application/UI Layer: run_app orchestrates the flow, calls the data layer once, processes data, and invokes AI functions.

Me: So ask_ollama focuses only on communicating with the LLM, while get_weather_data focuses only on weather API requests. This keeps each function small and testable.

AI: Exactly. This makes debugging easier, promotes code reuse, and allows each module to evolve independently.

Me: Can we also discuss streaming output for the user? Right now, the function returns the whole content at once. I want to maybe yield each chunk for real-time display in the console or UI.

AI: Sure. You can change the function to a generator. Replace output += ... with yield chunk['message']['content']. Then the caller can iterate over it and print each chunk immediately. You could keep a full_output variable to return the entire content at the end if needed.

Me: Thatâ€™s perfect. And regarding robustness, do we need to consider retries for transient network errors?

AI: Yes, you could wrap the requests.post call in a retry loop with exponential backoff. But for the first iteration, itâ€™s okay to just capture the exception and return a detailed error message. We can add retries later.

Me: Iâ€™ll add that in Phase 2. For now, I want to summarize what weâ€™ve achieved:

Streaming support implemented using iter_lines()

Robust JSON parsing with malformed line handling

Detailed error messages separating HTTP errors and network errors

Single Responsibility Principle applied for modular design

Plan for generator/yield support for real-time output

AI: Thatâ€™s an excellent summary. It shows careful research and step-by-step improvement. You can now integrate this LLM utility into your application confidently.

Me: Great. Next, Iâ€™ll implement parse_weather_data to feed clean data to this LLM function. This conversation will be part of the development documentation for the utility function.

AI: Perfect. This conversation demonstrates iterative refinement, research on edge cases, and application of modular design principles. It clearly justifies each coding decision.

Me: Awesome. I feel confident that the WeatherWise LLM utility is now production-ready for the next phase.

AI: Indeed. Your approach balances robustness, modularity, and future extensibilityâ€”a strong foundation for the full application.

Me: â€œI notice we always call requests.post directly. Should we use a requests.Session with retry logic for network resilience?â€
AI: â€œGood point. Using requests.Session with urllib3.util.retry.Retry allows automatic retries for transient errors like timeouts or 5xx responses. You can configure backoff_factor to avoid overwhelming the server. This makes ask_ollama much more robust in production.â€

from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

session = requests.Session()
retries = Retry(total=3, backoff_factor=2, status_forcelist=[500,502,503,504])
adapter = HTTPAdapter(max_retries=retries)
session.mount("http://", adapter)
session.mount("https://", adapter)

Me: â€œEarlier we talked about yielding chunks. Can you show a concrete generator implementation?â€
AI: â€œSure, instead of accumulating output in a string, we can yield each content piece. The caller can iterate and display in real-time. You can still collect the full output if needed.â€

def ask_ollama_stream(messages: list):
    response = requests.post(url, headers=headers, json=data, stream=True)
    response.raise_for_status()
    for line in response.iter_lines():
        if line:
            try:
                chunk = json.loads(line.decode('utf-8'))
                if "message" in chunk and "content" in chunk["message"]:
                    yield chunk["message"]["content"]
                if chunk.get("done", False):
                    break
            except json.JSONDecodeError:
                continue

Me: â€œShould we make type hints more explicit?â€
AI: â€œYes, for maintainability. For example, the generator returns Iterator[str], while the main function could return str or dict for errors. It helps IDEs and static analysis.â€

from typing import Iterator, Union

def ask_ollama_stream(messages: list) -> Iterator[str]:
    ...

Me: â€œRight now errors are just returned. Should we log them?â€
AI: â€œDefinitely. Using Pythonâ€™s logging module allows info, warning, and error levels. This is essential for production debugging and monitoring.â€

import logging
logging.basicConfig(level=logging.INFO)
...
logging.error(f"HTTP Error {e.response.status_code}: {e.response.reason}")

Me: â€œShould we validate the structure of streamed chunks?â€
AI: â€œYes, at least minimal validation prevents KeyError. For example, chunk.get('message', {}).get('content') safely extracts content even if fields are missing. For larger projects, a library like pydantic can enforce schemas.â€

Me: â€œCan we design it so we can mock streaming responses for unit tests?â€

AI: â€œAbsolutely. Make the session or requests.post injectable. In tests, you can replace it with a mocked object that yields JSON lines. This enables rigorous edge case testing without hitting a real server.â€

