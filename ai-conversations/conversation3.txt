Convesation3. Exponential Backoff & Non-Retryable Error Handling

Tool: ChatGPT, Google Gemini
Date: 2025-10-15

Me: I want a weather-fetching function that’s very reliable. It should retry temporary network failures, but fail immediately on 404 because that means the location doesn’t exist. Also, I want exponential backoff with jitter to avoid retry storms.

AI: Okay. We can structure it in stages: a retry loop, exponential backoff with jitter, distinguish retryable vs non-retryable errors, and return a consistent error dictionary.

Me: Let’s start with a simple retry loop. I don’t care about jitter yet.

MAX_RETRIES = 5

for attempt in range(MAX_RETRIES):
    try:
        response = HTTP_GET(API_URL, timeout=10)
        response.raise_for_status()
        return response.json()
    
    except HTTPError:
        handle_error = True
    
    except NetworkError:
        handle_error = True
    
    if handle_error and attempt < MAX_RETRIES - 1:
        SLEEP(2 ** attempt)


AI: This gives a basic retry loop, but it doesn’t handle non-retryable errors like 404, and there’s no jitter or error logging.

Me: I want random jitter added to prevent retry storms.

wait_time = (2 ** attempt) + random.uniform(0, 1)
SLEEP(wait_time)


AI: Perfect. Exponential backoff 2^attempt plus jitter [0,1] seconds. This makes retries gradually increasing and spread out.

Me: Retry for a 404 is pointless. How do we immediately fail for non-retryable errors?

AI: Inside the HTTPError block, check the status code:

if e.response.status_code == 404:
    return {"error": 404, "message": f"Location '{location}' not found or invalid."}


Me: Great. What about 5xx errors or network issues?

AI: Treat them as retryable. Use RequestException to catch all network-related errors and handle them in the same retry loop with backoff.

Me: Can we combine HTTPError and other network exceptions?

AI: Yes. HTTPError is a subclass of RequestException. So even if you just catch RequestException, you can still inspect e.response.status_code safely. That way, you catch all network failures but still detect 404 specifically.

Me: Let’s put this together into a complete Python function.

import requests
import time
import random
from requests.exceptions import RequestException, HTTPError
from typing import Dict

def get_weather_data(location: str, max_retries: int = 5) -> Dict:
    url = f"https://wttr.in/{location}?format=j1"
    
    for attempt in range(max_retries):
        error_message = ""
        
        try:
            print(f"Fetching data for '{location}'... (Attempt {attempt + 1}/{max_retries})")
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            return response.json()
        
        except HTTPError as e:
            if e.response.status_code == 404:
                print(f"❌ ERROR 404: Location '{location}' not found. Exiting immediately.")
                return {"error": 404, "message": f"Location '{location}' not found or invalid."}
            error_message = f"HTTP Status Error: {e.response.status_code} {e.response.reason}"
        
        except RequestException as e:
            error_message = f"Network/Connection Failure: {type(e).__name__}"
        
        if attempt < max_retries - 1:
            wait_time = (2 ** attempt) + random.uniform(0, 1)
            print(f"⚠️ Transient Error: {error_message}. Retrying in {wait_time:.2f} seconds.")
            time.sleep(wait_time)
        else:
            print(f"❌ Max retries reached ({max_retries}). Final Error: {error_message}")
            return {"error": 503, "message": f"Failed to retrieve data after {max_retries} attempts. Cause: {error_message}"}
    
    return {"error": 500, "message": "Unexpected application failure."}


Me: Looks good. Can you explain why checking e.response.status_code == 404 in the RequestException block is enough, without having a separate HTTPError block?

AI: Sure. Python’s exception hierarchy helps here. HTTPError is a subclass of RequestException. Every HTTPError has a response object. So by catching RequestException, you can catch network issues and HTTP errors together, and safely check e.response.status_code for 404. This makes the code more concise while still handling both retryable and non-retryable errors.

Me: Excellent. This handles exponential backoff, jitter, retryable network errors, and non-retryable 404 errors. It’s very robust.

AI: Exactly. This ensures efficient use of resources, avoids unnecessary retries on 404, and provides consistent error dictionaries for all failure cases. The function is now production-ready for reliable weather data fetching.

Me: I want a weather-fetching function that’s really robust. It should retry temporary network failures but immediately fail on 404 because that means the location doesn’t exist. I also want exponential backoff with jitter to avoid overwhelming the server.

AI: Okay, so we need a retry loop with clear distinctions between retryable and non-retryable errors, and proper logging. Let’s start with the basic structure.

Me: Let’s just start with a simple loop that retries on any error, without worrying about jitter or specific HTTP codes yet.

MAX_RETRIES = 5

for attempt in range(MAX_RETRIES):
    try:
        response = HTTP_GET(API_URL, timeout=10)
        response.raise_for_status()
        return response.json()
    
    except HTTPError:
        handle_error = True
    
    except NetworkError:
        handle_error = True
    
    if handle_error and attempt < MAX_RETRIES - 1:
        SLEEP(2 ** attempt)


AI: That works as a basic retry loop. But right now, it retries on all errors, including 404, which we don’t want. Also, no logging, no jitter.

Me: Right. Let’s add jitter to prevent retry storms. Something like wait_time = 2**attempt + random.uniform(0,1).

AI: Exactly. That gradually increases wait time while adding randomness. Also, we should log which attempt we’re on and the error details for better observability.

Me: Okay, let’s talk about 404. It’s non-retryable because the location doesn’t exist. How do we handle that?

AI: In the HTTPError block, check e.response.status_code. If it’s 404, we return a standard error dictionary and exit immediately.

if e.response.status_code == 404:
    return {"error": 404, "message": f"Location '{location}' not found or invalid."}


Me: Got it. And 5xx or network issues should still retry, right?

AI: Yes. All transient issues—5xx or connection problems—should go through the exponential backoff loop.

Me: Can we merge HTTPError and other network exceptions into one block for simplicity?

AI: Yes. HTTPError is a subclass of RequestException. So if we catch RequestException, we can still check e.response.status_code for 404. All other network errors are handled the same way.

Me: Cool. Let’s try putting it all together in a complete function with logging, exponential backoff, jitter, retryable network errors, and 404 handling.

import requests
import time
import random
from requests.exceptions import RequestException, HTTPError
from typing import Dict

def get_weather_data(location: str, max_retries: int = 5) -> Dict:
    url = f"https://wttr.in/{location}?format=j1"
    
    for attempt in range(max_retries):
        error_message = ""
        
        try:
            print(f"Fetching data for '{location}'... (Attempt {attempt + 1}/{max_retries})")
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            return response.json()
        
        except HTTPError as e:
            if e.response.status_code == 404:
                print(f"❌ ERROR 404: Location '{location}' not found. Exiting immediately.")
                return {"error": 404, "message": f"Location '{location}' not found or invalid."}
            error_message = f"HTTP Status Error: {e.response.status_code} {e.response.reason}"
        
        except RequestException as e:
            error_message = f"Network/Connection Failure: {type(e).__name__}"
        
        if attempt < max_retries - 1:
            wait_time = (2 ** attempt) + random.uniform(0, 1)
            print(f"⚠️ Transient Error: {error_message}. Retrying in {wait_time:.2f} seconds.")
            time.sleep(wait_time)
        else:
            print(f"❌ Max retries reached ({max_retries}). Final Error: {error_message}")
            return {"error": 503, "message": f"Failed to retrieve data after {max_retries} attempts. Cause: {error_message}"}
    
    return {"error": 500, "message": "Unexpected application failure."}


Me: Looks solid. But I want to understand: why is checking e.response.status_code == 404 inside the RequestException block enough? Why not separate HTTPError?

AI: Because in Python, HTTPError is a subclass of RequestException. Every HTTPError has a response object, so you can safely check status_code. Catching RequestException ensures that network issues like ConnectionError or Timeout are handled too. This makes the code more concise while still clearly separating retryable and non-retryable errors.

Me: Makes sense. I also want to log exactly which type of failure happened—HTTP error vs network error—so I can monitor reliability over time.

AI: Already covered in the error_message variable. For HTTP errors, it logs status code and reason. For network issues, it logs the exception type. Each retry will print that along with the wait time.

Me: Excellent. One last thing—if somehow the loop completes without returning, we should have a final fallback error, right?

AI: Yes. That’s the final return at the bottom: {"error": 500, "message": "Unexpected application failure."}. In practice, it won’t be reached because all cases are handled, but it’s a safety net.

Me: Perfect. So now we have a function that: retries transient failures with exponential backoff and jitter, immediately fails on 404, logs every attempt and error, and returns consistent error dictionaries. This is exactly what I needed.

AI: Exactly. This ensures efficient resource usage, avoids pointless retries on non-existent locations, and provides robust handling of temporary network errors. It’s ready for production use.

Me: I want a weather-fetching function that’s really robust. It should retry temporary network failures but immediately fail on 404 because that means the location doesn’t exist. Exponential backoff with jitter is a must to avoid hammering the server.

AI: Okay, let’s start with the basic retry loop structure. We’ll handle temporary network errors first and leave 404 handling for a later step.

Me: Sure, just the skeleton loop to begin with:

MAX_RETRIES = 5
for attempt in range(MAX_RETRIES):
    try:
        response = HTTP_GET(API_URL, timeout=10)
        response.raise_for_status()
        return response.json()
    except Exception:
        handle_error = True
    if handle_error:
        SLEEP(2 ** attempt)


Me: Looks simple, but it blindly retries everything. That’s not ideal.

AI: Right. Not all errors are retryable. Network errors like timeouts and server-side 5xx errors should retry. Client-side 404 shouldn’t.

Me: Let’s first make it explicit which exceptions we want to catch for retry. Maybe RequestException covers network problems, and HTTPError for HTTP status issues.

AI: Exactly. And HTTPError is a subclass of RequestException, so we can catch RequestException and check response.status_code to distinguish errors.

Me: Okay, let’s implement that. But before 404 handling, I want logging for each attempt. Knowing which attempt failed and why is important.

for attempt in range(MAX_RETRIES):
    try:
        print(f"Attempt {attempt+1} fetching '{location}'")
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.json()
    except RequestException as e:
        print(f"Error occurred: {type(e).__name__}")
        handle_error = True


Me: Good. Now for 404, I think I should immediately exit the loop, right?

AI: Yes. Inside the except RequestException block, check if hasattr(e, "response") and e.response.status_code == 404:. If true, return a standard error dictionary immediately.

Me: Let’s write that.

if hasattr(e, "response") and e.response.status_code == 404:
    print(f"❌ ERROR 404: Location '{location}' not found. Exiting immediately.")
    return {"error": 404, "message": f"Location '{location}' not found or invalid."}


Me: Cool. For other errors—network timeouts, 5xx—we retry with exponential backoff and jitter.

AI: Yes. Wait time can be 2 ** attempt + random.uniform(0,1). Logging before sleeping is also important.

wait_time = (2 ** attempt) + random.uniform(0, 1)
print(f"⚠️ Transient Error. Retrying in {wait_time:.2f} seconds...")
time.sleep(wait_time)


Me: Makes sense. But if retries exceed MAX_RETRIES, we need a final error message with cause.

print(f"❌ Max retries reached ({MAX_RETRIES}). Final Error: {error_message}")
return {"error": 503, "message": f"Failed after {MAX_RETRIES} attempts. Cause: {error_message}"}


Me: I wonder—should we combine HTTPError and network exceptions into one block?

AI: Yes, because HTTPError is already a subclass of RequestException. This keeps the code concise while still handling all cases correctly.

Me: Now, let’s put everything together, step by step, making sure logging, exponential backoff, jitter, retryable errors, and 404 handling are all in place.

import requests
import time
import random
from requests.exceptions import RequestException, HTTPError
from typing import Dict

def get_weather_data(location: str, max_retries: int = 5) -> Dict:
    url = f"https://wttr.in/{location}?format=j1"
    
    for attempt in range(max_retries):
        error_message = ""
        
        try:
            print(f"Fetching data for '{location}'... (Attempt {attempt + 1}/{max_retries})")
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            return response.json()
        
        except RequestException as e:
            if hasattr(e, "response") and e.response and e.response.status_code == 404:
                print(f"❌ ERROR 404: Location '{location}' not found. Exiting immediately.")
                return {"error": 404, "message": f"Location '{location}' not found or invalid."}
            
            if hasattr(e, "response") and e.response:
                error_message = f"HTTP Status Error: {e.response.status_code} {e.response.reason}"
            else:
                error_message = f"Network/Connection Failure: {type(e).__name__}"
        
        if attempt < max_retries - 1:
            wait_time = (2 ** attempt) + random.uniform(0, 1)
            print(f"⚠️ Transient Error: {error_message}. Retrying in {wait_time:.2f} seconds.")
            time.sleep(wait_time)
        else:
            print(f"❌ Max retries reached ({max_retries}). Final Error: {error_message}")
            return {"error": 503, "message": f"Failed to retrieve data after {max_retries} attempts. Cause: {error_message}"}
    
    return {"error": 500, "message": "Unexpected application failure."}


Me: Perfect. Each step is logged, 404 exits immediately, transient network and 5xx errors retry with exponential backoff and jitter, and there’s a final safety net.

AI: Exactly. This function is now robust for production use, handles edge cases efficiently, avoids unnecessary retries, and provides detailed logs for monitoring.

Me: I like how concise it is now, even though it handles many edge cases. Also, catching RequestException alone is enough to differentiate 404 from other errors, thanks to Python’s exception hierarchy.

AI: Right. HTTPError is a subclass of RequestException, and every HTTPError carries a response object, so checking status_code inside RequestException works perfectly.

Me: Great. I think this fully satisfies the requirements: robustness, clear retry logic, immediate 404 exit, jitter, exponential backoff, and detailed logging.

AI: Exactly. It’s an elegant, production-ready solution.
